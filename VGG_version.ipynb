{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nimport numpy as np\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# Any results you write to the current directory are saved as output.\n# Set the seed for random operations. \n# This let our experiments to be reproducible. \nSEED = 1234\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# Get current working directory\ncwd = os.getcwd()\n\n# Set GPU memory growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","execution_count":2,"outputs":[{"output_type":"stream","text":"1 Physical GPUs, 1 Logical GPUs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare envinromment useful variables\n\n# Get current working directory\ncwd = os.path.join(\"/kaggle/input\")\n\ndataset_dir = os.path.join(cwd, 'ann-and-dl-vqa/dataset_vqa')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n#open Dataset from directory\nwith open(os.path.join(dataset_dir,'train_data.json'), 'r') as f:\n    train_data_jsonload = json.load(f)\nf.close()\ntrain_data = train_data_jsonload.get(\"questions\")\n\n#print(train_data)\n\n#Qui ho inserito il validation split. NB: dobbiamo settare per bene e coerentemente tutte le dimensioni di length max + padding (nella parte di tokenizer è un puttanaio, va ripulito). \n#Al momento se inserisco il validation split, la lunghezza massima si riduce a 40 (la domanda lunga 41 finisce nel validation probabilmente) e di conseguenza nel processo di padding delle stringhe\n#paddo la lunghezza fino a 40, che va in conflitto con l'input della rete che si aspetta una lunghezza fissa di 41 (che è quella giusta)\n\ntrain_data[0]\ndataset_size=len(train_data)\n\ntrain_size = round((dataset_size/100)*80)\nprint(train_size)\n\nvalidation_size = dataset_size-train_size\nprint(validation_size)\n\nvalidation_data = train_data[train_size:]\nprint(len(validation_data))\n\ntrain_data2=train_data[:train_size]\nprint(len(train_data))\n","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"{'answer': '2',\n 'image_filename': 'CLEVR_train_000000.png',\n 'question': 'How many other things are there of the same shape as the tiny cyan matte object?'}"},"metadata":{}},{"output_type":"stream","text":"207594\n51898\n51898\n259492\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Qui creo la lista di domande, per intera, sulla quale usare il tokenizer\n#varie print e cazzate di testing\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nquestions_list =list() #lista contenente tutte le domande \nfor element in train_data:\n    questions_list.append(element.get(\"question\"))\n    #print(element.get(\"question\"))\n    \n#print('printo la mia lista di domande')\n#print(questions_list)\n\n#Preparing data with tokenizer\ntokenizer=Tokenizer()\ntokenizer.fit_on_texts(questions_list)\nvocab_size = len(tokenizer.word_index) + 1\n\nprint('printo la mia domanda più lunga')\nlongest_question=max(questions_list, key=len)\nprint(longest_question)\n\nmax_ita_length = max(len(sentence) for sentence in questions_list)\nprint('testing length:',max_ita_length)\nlongest_question_tokenized = tokenizer.texts_to_sequences(longest_question)\nprint('printo la mia domanda più lunga tokenizzata')\nprint(longest_question_tokenized)\n\ntest_list = list()\ntest_list.append('What number of objects are either brown things that are on the right side of the tiny brown metal cylinder or large brown matte cubes that are on the right side of the large purple rubber cylinder?')\ntest=tokenizer.texts_to_sequences(test_list) #texts_to_sequences ti aspetta una list di stringhe, credo \nprint('printo test:')\nprint(test)\n\nprint('printo la lunghezza di test')\nprint(len(test[0]))\n\n\n\n\nprint(\"vocab size:\")\nprint(vocab_size)\n\nprint(\"word_counts:\")\nprint(tokenizer.word_counts) #mi printa le parole e il numero di volte che sono apparse in totale \n\nprint(\"document_count:\")\nprint(tokenizer.document_count)\n\nprint(\"word_indexes:\")\nprint(tokenizer.word_index)#printa la parola e l'indice che gli è stato associato\n\nquestion_tokenized = tokenizer.texts_to_sequences(questions_list)\n\nita_wtoi = tokenizer.word_index\n#print('Total italian words:', len(ita_wtoi))\n\nmax_question_length = max(len(sentence) for sentence in question_tokenized)\nprint('Max question length:', max_question_length)\n\nlongest_question=max(question_tokenized, key=len)\nprint('max length tokenized')\nprint(longest_question)\n#la lunghezza massima di una domanda è di 41 parole","execution_count":5,"outputs":[{"output_type":"stream","text":"printo la mia domanda più lunga\nWhat number of objects are either brown things that are on the right side of the tiny brown metal cylinder or large brown matte cubes that are on the right side of the large purple rubber cylinder?\ntesting length: 197\nprinto la mia domanda più lunga tokenizzata\n[[], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [27], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\nprinto test:\n[[13, 14, 2, 9, 3, 45, 36, 7, 10, 3, 56, 1, 40, 57, 2, 1, 16, 36, 30, 26, 23, 20, 36, 19, 60, 10, 3, 56, 1, 40, 57, 2, 1, 20, 32, 18, 26]]\nprinto la lunghezza di test\n37\nvocab size:\n71\nword_counts:\nOrderedDict([('how', 82962), ('many', 82962), ('other', 60726), ('things', 112901), ('are', 221072), ('there', 138746), ('of', 227440), ('the', 416141), ('same', 125584), ('shape', 31306), ('as', 125584), ('tiny', 68664), ('cyan', 39532), ('matte', 66977), ('object', 66149), ('gray', 38945), ('on', 23749), ('right', 35455), ('side', 23749), ('small', 68481), ('rubber', 67078), ('cube', 26681), ('behind', 35245), ('large', 66161), ('brown', 39464), ('thing', 76590), ('left', 35425), ('any', 57284), ('that', 91280), ('have', 28261), ('size', 31364), ('shiny', 45041), ('sphere', 27034), ('cubes', 20831), ('yellow', 39557), ('block', 26793), ('green', 38622), ('blue', 39543), ('what', 82444), ('number', 82444), ('red', 39172), ('spheres', 20890), ('is', 107830), ('cylinder', 53570), ('a', 47299), ('metal', 44897), ('cylinders', 41034), ('in', 35182), ('front', 35182), ('it', 23752), ('ball', 26639), ('balls', 20667), ('metallic', 45188), ('or', 63214), ('big', 65783), ('blocks', 20802), ('color', 31466), ('purple', 39664), ('objects', 101680), ('made', 8724), ('material', 31448), ('visible', 2620), ('both', 3962), ('to', 23669), ('and', 7932), ('another', 2705), ('has', 17361), ('either', 31751), ('anything', 10550), ('else', 10550)])\ndocument_count:\n259492\nword_indexes:\n{'the': 1, 'of': 2, 'are': 3, 'there': 4, 'same': 5, 'as': 6, 'things': 7, 'is': 8, 'objects': 9, 'that': 10, 'how': 11, 'many': 12, 'what': 13, 'number': 14, 'thing': 15, 'tiny': 16, 'small': 17, 'rubber': 18, 'matte': 19, 'large': 20, 'object': 21, 'big': 22, 'or': 23, 'other': 24, 'any': 25, 'cylinder': 26, 'a': 27, 'metallic': 28, 'shiny': 29, 'metal': 30, 'cylinders': 31, 'purple': 32, 'yellow': 33, 'blue': 34, 'cyan': 35, 'brown': 36, 'red': 37, 'gray': 38, 'green': 39, 'right': 40, 'left': 41, 'behind': 42, 'in': 43, 'front': 44, 'either': 45, 'color': 46, 'material': 47, 'size': 48, 'shape': 49, 'have': 50, 'sphere': 51, 'block': 52, 'cube': 53, 'ball': 54, 'it': 55, 'on': 56, 'side': 57, 'to': 58, 'spheres': 59, 'cubes': 60, 'blocks': 61, 'balls': 62, 'has': 63, 'anything': 64, 'else': 65, 'made': 66, 'and': 67, 'both': 68, 'another': 69, 'visible': 70}\nMax question length: 41\nmax length tokenized\n[4, 8, 27, 17, 39, 30, 51, 10, 8, 56, 1, 40, 57, 2, 1, 20, 15, 10, 8, 56, 1, 41, 57, 2, 1, 35, 18, 52, 3, 4, 25, 17, 37, 19, 31, 10, 3, 43, 44, 2, 55]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ImageDataGenerator\n# ------------------\n#Per la parte di stringhe, usare tokenizer. Prima estriamo TUTTE le domandee su di esse runniamo \n#tokenizer = Tokenizer()\n#tokenizer.fit_on_texts(text)\n#e otteniamo la tokenizzazione di tutte le parole uniche nelle nostre domande.\n#Una volta fatto cio, possiamo passare tranquillamente al generator usando, per ogni stringa generata dal generator il metodo sequences = tokenizer.texts_to_sequences(question)\n#che returna la stessa stringa ma con degli interi che indicano le parole.\n#Fonte: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n\n#NB: Da verificare se non c'è da applicare prima un text processing (rimozione punteggiatura,maiuscole ecc...), che però possiamo embeddare nel tokenizer in teoria \n#-----------------\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nimport pandas as pd\n\nimg_directory= os.path.join(dataset_dir, \"train\")\nimport cv2 \n\nthisdict = {\n  '0': 0,\n  '1': 1,\n '10': 2,\n  '2': 3,\n  '3': 4,\n  '4': 5,\n  '5': 6,\n  '6': 7,\n  '7': 8,\n  '8': 9,\n  '9': 10,\n 'no': 11,\n'yes': 12\n}\n\n\n\ndef get_image(question):\n    image_name = question.get(\"image_filename\")\n    img = cv2.imread(os.path.join(img_directory, image_name))\n    return(img)\n\ndef get_question(question):\n    question_text = question.get(\"question\")\n    return(question_text)\n\ndef question_to_token(question):\n    question_list = list()\n    question_list.append(question)\n    question_token_list = tokenizer.texts_to_sequences(question_list)\n    return(question_token_list[0])\n\ndef padding(question_token):\n    # Pad to max question length\n    question_token_list = list()\n    question_token_list.append(question_token)\n    question_padded_list = pad_sequences(question_token_list, maxlen=max_question_length, padding='post')\n    return(question_padded_list[0])\n\ndef preprocess_question(question):\n    question_tokenized = question_to_token(question)\n    question_padded = padding(question_tokenized)\n    return(question_padded)\n\n\ndef get_output(question):\n    question_answer = question.get(\"answer\")\n    x = thisdict[question_answer]\n    return(x)\n\ndef preprocess_input(image):\n    \"\"\"--- Rescale Image\n    --- Rotate Image\n    --- Resize Image\n    --- Flip Image\n    --- PCA etc. \"\"\"   \n    return(image)\n\ndef image_generator(data, batch_size = 64):\n    \n    while True:\n        # Select files (paths/indices) for the batch\n        batch_questions = np.random.choice(a = data, \n                                         size = batch_size)\n        batch_input = []\n        batch_output = [] \n        batch_input_image=[]\n        batch_input_question=[]\n        \n        # Read in each input, perform preprocessing and get labels\n        for input_element in batch_questions:\n            input_question = get_question(input_element)\n            input_question_preprocessed = preprocess_question(input_question)\n            input_image = get_image(input_element)\n            output = get_output(input_element) \n            input_question_preprocessed\n            batch_input_image += [input_image]\n            batch_input_question += [input_question_preprocessed]\n            batch_output += [ output ]\n          # Return a tuple of (input,output) to feed the network\n        batch_x_image = np.array( batch_input_image )\n        batch_x_question = np.array( batch_input_question )\n        batch_y = np.array( batch_output )\n        yield( [batch_x_image,batch_x_question], batch_y )\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.applications import VGG16\n\n# img shape\nimg_h = 320\nimg_w = 480\n# ----------------------\nEMBEDDING_SIZE = 128\n\n# Define CNN for Image Input\n\nvision_model = Sequential()\n'''\nvision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_h, img_w, 3))) #Probabile errore di dimensioni qui \nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Flatten())'''\n\nvision_model.add(VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3)))\nvision_model.add(Flatten())\nimage_input = Input(shape=(img_h, img_w, 3))\nencoded_image = vision_model(image_input)\n\n# Define RNN for language input\nquestion_input = Input(shape=[41], dtype='int32')\nembedded_question = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=EMBEDDING_SIZE, input_length=41)(question_input)\nhidden_layer = LSTM(128,return_sequences=True)(embedded_question)\nencoded_question = LSTM(128)(hidden_layer)\n\n\n# Combine CNN and RNN to create the final model\nmerged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n#intermediate_1 = Dense(512, activation='relu')(merged)\nintermediate = Dense(256, activation='relu')(merged)\noutput = Dense(13, activation='softmax')(intermediate)\nvqa_model = Model(inputs=[image_input, question_input], outputs=output)   ","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"\"\\nvision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(img_h, img_w, 3))) #Probabile errore di dimensioni qui \\nvision_model.add(MaxPooling2D((2, 2)))\\nvision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\\nvision_model.add(MaxPooling2D((2, 2)))\\nvision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\\nvision_model.add(MaxPooling2D((2, 2)))\\nvision_model.add(Flatten())\""},"metadata":{}},{"output_type":"stream","text":"Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58892288/58889256 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Early Stopping\n# --------------\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks=[]\nearly_stop = False\nif early_stop:\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20)\n    callbacks.append(es_callback)","execution_count":8,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = ['accuracy']\nvqa_model.compile(loss='sparse_categorical_crossentropy',metrics= metrics, optimizer = 'rmsprop')\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nvqa_model.summary()\n#plot_model(vqa_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 41)]         0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 41, 128)      9088        input_3[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 41, 128)      131584      embedding[0][0]                  \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 320, 480, 3) 0                                            \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 128)          131584      lstm[0][0]                       \n__________________________________________________________________________________________________\nsequential (Sequential)         (None, 76800)        14714688    input_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 76928)        0           lstm_1[0][0]                     \n                                                                 sequential[1][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 256)          19693824    concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 13)           3341        dense[0][0]                      \n==================================================================================================\nTotal params: 34,684,109\nTrainable params: 34,684,109\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vqa_model.fit_generator(generator=image_generator(train_data2), callbacks=callbacks,validation_data=image_generator(validation_data), validation_steps=30, epochs=80, steps_per_epoch=30)  #fix this\n","execution_count":11,"outputs":[{"output_type":"stream","text":"Train for 30 steps, validate for 30 steps\nEpoch 1/80\n30/30 [==============================] - 75s 2s/step - loss: 3119.6175 - accuracy: 0.2016 - val_loss: 2.2203 - val_accuracy: 0.2083\nEpoch 2/80\n30/30 [==============================] - 61s 2s/step - loss: 2.0043 - accuracy: 0.1911 - val_loss: 1.9270 - val_accuracy: 0.1813\nEpoch 3/80\n30/30 [==============================] - 61s 2s/step - loss: 1.9731 - accuracy: 0.1979 - val_loss: 1.8946 - val_accuracy: 0.1714\nEpoch 4/80\n30/30 [==============================] - 61s 2s/step - loss: 1.9236 - accuracy: 0.1953 - val_loss: 1.9493 - val_accuracy: 0.2146\nEpoch 5/80\n30/30 [==============================] - 61s 2s/step - loss: 7.0412 - accuracy: 0.1932 - val_loss: 1.9109 - val_accuracy: 0.2130\nEpoch 6/80\n30/30 [==============================] - 61s 2s/step - loss: 1.8480 - accuracy: 0.2438 - val_loss: 1.4653 - val_accuracy: 0.4005\nEpoch 7/80\n30/30 [==============================] - 60s 2s/step - loss: 1.4347 - accuracy: 0.3870 - val_loss: 1.4047 - val_accuracy: 0.4026\nEpoch 8/80\n30/30 [==============================] - 60s 2s/step - loss: 1.4536 - accuracy: 0.3438 - val_loss: 1.4319 - val_accuracy: 0.3354\nEpoch 9/80\n30/30 [==============================] - 60s 2s/step - loss: 1.3856 - accuracy: 0.3724 - val_loss: 1.3605 - val_accuracy: 0.4073\nEpoch 10/80\n30/30 [==============================] - 60s 2s/step - loss: 1.3747 - accuracy: 0.3964 - val_loss: 1.4060 - val_accuracy: 0.3802\nEpoch 11/80\n30/30 [==============================] - 60s 2s/step - loss: 1.3838 - accuracy: 0.3922 - val_loss: 1.4405 - val_accuracy: 0.3297\nEpoch 12/80\n16/30 [===============>..............] - ETA: 29s - loss: 1.4218 - accuracy: 0.3667","name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":" OOM when allocating tensor with shape[64,64,320,480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Conv2DBackpropInput_11 (defined at <ipython-input-11-6711b4e55504>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_7448]\n\nFunction call stack:\ndistributed_function\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6711b4e55504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#fix this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m   def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,64,320,480] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Conv2DBackpropInput_11 (defined at <ipython-input-11-6711b4e55504>:1) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_7448]\n\nFunction call stack:\ndistributed_function\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport operator\nfrom datetime import datetime\nfrom PIL import Image\nimport json\n\ndef create_csv(results, results_dir='./'):\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n        f.write('Id,Category\\n')\n        for key, value in results.items():\n            f.write(str(key) + ',' + str(value) + '\\n')\n\n#open Dataset from directory\nwith open(os.path.join(dataset_dir,'test_data.json'), 'r') as f:\n    test_data_jsonload = json.load(f)\n    f.close()\ntest_data = test_data_jsonload.get(\"questions\")\n\nfrom datetime import datetime\nfrom PIL import Image\nIMG_DIM = (320, 480)\ntest_img_directory=os.path.join(dataset_dir, 'test')\nimport cv2\ndef get_image(question):\n    image_name = question.get(\"image_filename\")\n    img = cv2.imread(os.path.join(test_img_directory, image_name))\n    return(img)\ndef get_question(question):\n    question_text = question.get(\"question\")\n    return(question_text)\ndef question_to_token(question):\n    question_list = list()\n    question_list.append(question)\n    question_token_list = tokenizer.texts_to_sequences(question_list)\n    return(question_token_list[0])\ndef padding(question_token):\n    # Pad to max question length\n    question_token_list = list()\n    question_token_list.append(question_token)\n    question_padded_list = pad_sequences(question_token_list, maxlen=max_question_length, padding='post')\n    return(question_padded_list[0])\ndef preprocess_question(question):\n    question_tokenized = question_to_token(question)\n    question_padded = padding(question_tokenized)\n    return(question_padded)\nresults={}\nindex=0\n\nfor question in test_data:\n    question_string=preprocess_question(get_question(question))\n    image = get_image(question)\n    x=[image,question_string]\n    #print('printo x')\n    #print(x)\n    #x.shape\n    #https://stackoverflow.com/questions/41563720/error-when-checking-model-input-expected-convolution2d-input-1-to-have-4-dimens\n    image = np.expand_dims(image, axis=0)\n    out_softmax = vqa_model.predict(x=(image,np.array([question_string])))\n    print(out_softmax) \n    out_softmax =  np.argmax(out_softmax)\n    print('printo index')\n    print(out_softmax)\n    #out_softmax = np.asscalar(out_softmax)\n    #print(out_softmax)\n    #print('printo il valore di question')\n    #print(question)\n    #da correggere, la variabile name non esiste. Che indice devo usare? \n    results[question.get('question_id')] = out_softmax\ncreate_csv(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}